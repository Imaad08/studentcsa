{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: Jupyter Hacks\n",
    "comments: true\n",
    "---\n",
    "\n",
    "# Emoji play\n",
    "First `pip install emoji`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç Python is awesome! üòÄ\n"
     ]
    }
   ],
   "source": [
    "from emoji import emojize \n",
    "print(emojize(\":thumbs_up: Python is awesome! :grinning_face:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newspaper extraction\n",
    "`pip install newspaper3k`\n",
    "`pip install lxml_html_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "New release: iPhone 16 to make debut at Apple event"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "CNN ‚Äî\n",
       "\n",
       "Apple is expected to unveil the iPhone 16 Monday. Although the new phone will probably get a few small upgrades on the outside, Apple is betting that significant changes to what‚Äôs on the inside will excite customers who haven‚Äôt been given a convincing reason to upgrade in years.\n",
       "\n",
       "The company is set to introduce the first lineup of iPhones purpose-built for generative artificial intelligence (which lets users create text and images) during its annual hardware event, which kicks off at 1 p.m. ET.\n",
       "\n",
       "The event was teased with the cryptic motto of ‚Äúit‚Äôs glow time,‚Äù and Apple is staying mum thus far on what it means.\n",
       "\n",
       "Apple faces immense pressure ahead of the event to prove the new AI features and other iPhone 16 updates are worth shelling out for an upgrade. Apple also needs to convince investors that it hasn‚Äôt fallen behind in the AI arms race, as rivals have already released similar features with the new technology.\n",
       "\n",
       "Since the launch of the iPhone 12 with 5G connectivity in 2020, the company has given customers few reasons to buy the latest generation. Phone cameras have largely advanced to the point where they‚Äôre sufficient for most people‚Äôs day-to-day needs without major hardware changes, and at some point the human eye isn‚Äôt even capable of perceiving higher screen resolutions.\n",
       "\n",
       "As a result, iPhone sales, which make up half of the company‚Äôs revenue, have been sluggish.\n",
       "\n",
       "A man holds a bag with a new iPhone on September 22, 2023 when Apple's iPhone 15 went on sale in Shanghai, China. Aly Song/Reuters\n",
       "\n",
       "The company‚Äôs shares had been relatively stagnant, too, until the Apple Intelligence unveiling, a sign that investors are counting on AI to convince people to buy new iPhones. Apple‚Äôs stock is up 14% since the June 10 event, and nearly 18% since the start of this year.\n",
       "\n",
       "If Apple delivers, it could cash in ‚Äì roughly 300 million iPhones worldwide have not been upgraded in more than four years, according to a research note from analyst Dan Ives of investment firm Wedbush last month.\n",
       "\n",
       "So, while Apple will likely highlight changes to various products and services like AirPods or, even Apple TV+ on Monday, ‚Äúeverything is about the iPhone upgrade cycle. Everything else is subplot,‚Äù D.A. Davidson analyst Gil Luria told CNN.\n",
       "\n",
       "Subplot or main character, here is everything we‚Äôre expecting from the Apple event.\n",
       "\n",
       "The AI iPhone\n",
       "\n",
       "The company has already hinted at some of the things Apple Intelligence will be able to do: it will enable more natural conversations with Siri, help draft emails, make it easier to find specific moments in your photo albums and incorporate users‚Äô personal information into its responses. The company‚Äôs task on Monday will be to show iPhone users what that will look in real life.\n",
       "\n",
       "‚ÄúExpect to see demos about how, within your text chain, you can get summaries,‚Äù Luria said. ‚ÄúYou‚Äôll know why did Tiffany M have beef with Tiffany R, and you‚Äôll be able to ask that within the chat and get a response. Those are the kinds of things that will get people excited ‚Ä¶ to show some concrete examples of how folks will be able to use Apple Intelligence to do things they weren‚Äôt previously able to do.‚Äù\n",
       "\n",
       "And while it‚Äôs not unusual for new iPhones to get an updated processor chip, that change may be especially important this year to ensure the iPhone 16 can handle the increased data processing needs that will come with the new AI features without compromising battery life.\n",
       "\n",
       "Luria added that he thinks there could also be a subtle change to the appearance of the iPhone, such as a wider screen or updated edges, ‚Äúto drive home the point of an upgrade cycle.‚Äù\n",
       "\n",
       "Apple iPhone 15 series devices are displayed for sale as a customer purchases an iPhone at The Grove Apple retail store on release day in Los Angeles, California, on September 22, 2023. Patrick T. Fallon/AFP/Getty Images\n",
       "\n",
       "‚ÄúSomething distinct about the new iPhones that will communicate to consumers that, ‚ÄòI have the new iPhone and you don‚Äôt,‚Äô which was not the case for the last four years,‚Äù he said.\n",
       "\n",
       "The iPhone 16 is also set to feature a dedicated camera button, according to a report from Bloomberg‚Äôs Mark Gurman.\n",
       "\n",
       "The big question: price?\n",
       "\n",
       "A major question heading into Monday‚Äôs event is how Apple will price the iPhone 16 lineup. For the last four years, the starting price at launch for the new iPhones was $799.\n",
       "\n",
       "Apple enthusiasts have debated for years whether iPhone models should be cheaper, while investors would prefer maximum profit.\n",
       "\n",
       "Many analysts, including CFRA Research‚Äôs Angelo Zino, say Apple could modestly raise prices ‚Äúacross the board‚Äù for the iPhone lineup because of the new AI features and the cost to the company of delivering them.\n",
       "\n",
       "However, it almost certainly won‚Äôt be a major hike because ‚Äúthey don‚Äôt want to lose the (customer) enthusiasm because of sticker shock,‚Äù Luria said.\n",
       "\n",
       "Other devices\n",
       "\n",
       "Rumor has it that Apple may also announce updates to the Apple Watch and AirPods.\n",
       "\n",
       "The Apple Watch Series 10 is expected to be thinner than its predecessors but with a larger screen, and the company is also set to roll out new low-end and mid-tier AirPod offerings, Gurman reported, citing unnamed people familiar with the situation.\n",
       "\n",
       "Those updates would follow new software offerings for both devices that Apple announced at its annual developers conference in June.\n",
       "\n",
       "AirPods users will be able to answer or decline a call with just a nod or shake of their head. And new vital sign tracking on Apple Watch can notify users when they may be getting sick, based on signals like body temperature and heart rate.\n",
       "\n",
       "‚ÄìSamantha Kelly and Ramishah Maruf contributed to this report."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Nvidia is suddenly in trouble"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "CNN ‚Äî\n",
       "\n",
       "Nvidia, the AI chipmaking titan that was briefly the world‚Äôs most valuable company, has suddenly found itself in an unfamiliar position: a major rut.\n",
       "\n",
       "Nvidia (NVDA) had the worst day in the history of the stock market Tuesday, as measured by loss in total market value. Its 9.5% share price decline shaved a stunning $279 billion off the company‚Äôs value, far outpacing the previous record of $240 billion set by Meta in 2022.\n",
       "\n",
       "To put that shocking decline into context, only 27 companies on the planet are worth as much as Nvidia lost in value Tuesday. That $279 billion evaporation is worth more than all the shares of some of America‚Äôs biggest companies, including McDonald‚Äôs, Chevron and Pepsi.\n",
       "\n",
       "CEO Jensen Huang, who is Nvidia‚Äôs largest individual shareholder (and fifth-largest overall, counting institutional investors like BlackRock) personally lost $10 billion in wealth Tuesday from Nvidia‚Äôs sharp tumble.\n",
       "\n",
       "The company has been in decline since June 18, when it topped $3.3 trillion in value ‚Äî the highest for any public company. As the US economy begins to show some signs of stress, investors have grown skeptical of Nvidia and other AI stocks‚Äô sky-high valuations. Stock traders are worried that potential weakness in the economy could make companies think twice before investing in the promising but still risky and unproven technology.\n",
       "\n",
       "Despite blockbuster earnings last week, Nvidia‚Äôs somewhat more tepid outlook disappointed investors who were looking for more upside, and the stock fell.\n",
       "\n",
       "Nvidia has tumbled more than 20% since its June 18 peak. Microsoft, which has made huge bets on AI technology, has fallen 12% from its most recent peak. And TSMC, Nvidia‚Äôs chip manufacturer, has plunged 18% since mid-July.\n",
       "\n",
       "Meanwhile, Intel, once the world biggest chipmaker, has endured a 59% decline in its share price this year. That company faces its own unique challenges in its bid to remake itself and get into the AI game.\n",
       "\n",
       "Potential legal problems\n",
       "\n",
       "But Nvidia may face a different set of problems: The government is reportedly investigating it over potential antitrust violations.\n",
       "\n",
       "Much of Tuesday‚Äôs sharp decline was because the US Justice Department reportedly sent it a subpoena as part of an antitrust probe, according to Bloomberg. CNN could not independently verify the subpoena, and the Department of Justice declined to comment directly on a potential antitrust investigation.\n",
       "\n",
       "Nvidia on Wednesday afternoon said it has not received a subpoena from the Justice Department.\n",
       "\n",
       "‚ÄúWe have inquired with the US Department of Justice and have not been subpoenaed,‚Äù an Nvidia spokesperson said in a statement. ‚ÄúNonetheless, we are happy to answer any questions regulators may have about our business.‚Äù\n",
       "\n",
       "The Biden administration has been going hard after tech titans, launching probes and lobbing charges against Apple, Google and Amazon, among others. It‚Äôs unclear whether a Kamala Harris or Donald Trump administration would continue those cases, but both have criticized tech companies for various reasons during their campaigns.\n",
       "\n",
       "Nvidia lost another 1.7% Wednesday. The Nasdaq Composite, which tanked more than 3% Tuesday, fell 0.3% Wednesday.\n",
       "\n",
       "Still, AI bulls continue to believe in Nvidia. The stock remains up 118% this year and has a $2.7 trillion market valuation ‚Äî a close third behind Apple and Microsoft. Huang said last week that demand for its latest ‚ÄúBlackwell‚Äù AI chips ‚Äúfar exceeds its supply.‚Äù And even as competition grows, demand for Nvidia‚Äôs chips is growing, too.\n",
       "\n",
       "And the investments are paying off ‚Äî so far, at least ‚Äî Huang claims.\n",
       "\n",
       "‚ÄúPeople who are investing in Nvidia infrastructure are getting returns on it right away,‚Äù Huang said last week, noting that the company‚Äôs new graphics processing units, the GPU chips that power AI, process data so efficiently that they end up saving clients money quickly.\n",
       "\n",
       "That‚Äôs why bulls like Wedbush‚Äôs Dan Ives believe Nvidia‚Äôs stock decline presents a buying opportunity.\n",
       "\n",
       "‚ÄúNvidia has changed the tech and global landscape as its GPUs have become the new oil and gold in the IT landscape,‚Äù Ives said Tuesday in a note to investors.\n",
       "\n",
       "‚Äì CNN‚Äôs Ramishah Maruf contributed to this report\n",
       "\n",
       "Correction: Correction: An earlier version of this story incorrectly described TSMC's relationship with Nvidia. TSMC manufacturers chips for Nvidia."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "urls = [\"https://www.cnn.com/2024/09/09/tech/new-iphone-16-apple-release/index.html\", \n",
    "        \"https://www.cnn.com/2024/09/04/tech/nvidia-is-in-trouble/index.html\"]\n",
    "\n",
    "for url in urls:\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    # Jupyter Notebook Display\n",
    "    # print(article.title)\n",
    "    display(Markdown(article.title)) # Jupyter display only\n",
    "    display(Markdown(article.text)) # Jupyter display only\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python (programming language)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\n",
       "Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.\n",
       "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\n",
       "Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JavaScript\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "JavaScript (), often abbreviated as JS, is a programming language and core technology of the Web, alongside HTML and CSS. 99% of websites use JavaScript on the client side for webpage behavior.\n",
       "Web browsers have a dedicated JavaScript engine that executes the client code. These engines are also utilized in some servers and a variety of apps. The most popular runtime system for non-browser usage is Node.js.\n",
       "JavaScript is a high-level, often just-in-time compiled language that conforms to the ECMAScript standard. It has dynamic typing, prototype-based object-orientation, and first-class functions. It is multi-paradigm, supporting event-driven, functional, and imperative programming styles. It has application programming interfaces (APIs) for working with text, dates, regular expressions, standard data structures, and the Document Object Model (DOM).\n",
       "The ECMAScript standard does not include any input/output (I/O), such as networking, storage, or graphics facilities. In practice, the web browser or other runtime system provides JavaScript APIs for I/O.\n",
       "Although Java and JavaScript are similar in name, syntax, and respective standard libraries, the two languages are distinct and differ greatly in design."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install wikipedia\n",
    "import wikipedia \n",
    "from IPython.display import display, Markdown \n",
    "\n",
    "terms = [\"Python (programming language)\", \"JavaScript\"]\n",
    "for term in terms:\n",
    "    # Search for a page \n",
    "    result = wikipedia.search(term)\n",
    "    # Get the summary of the first result\n",
    "    summary = wikipedia.summary(result[0])\n",
    "    print(term) \n",
    "    # print(summary) # console display\n",
    "    display(Markdown(summary)) # Jupyter display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspeciting a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Article(object):\n",
      "    \"\"\"Article objects abstract an online news article page\n",
      "    \"\"\"\n",
      "    def __init__(self, url, title='', source_url='', config=None, **kwargs):\n",
      "        \"\"\"The **kwargs argument may be filled with config values, which\n",
      "        is added into the config object\n",
      "        \"\"\"\n",
      "        if isinstance(title, Configuration) or \\\n",
      "                isinstance(source_url, Configuration):\n",
      "            raise ArticleException(\n",
      "                'Configuration object being passed incorrectly as title or '\n",
      "                'source_url! Please verify `Article`s __init__() fn.')\n",
      "\n",
      "        self.config = config or Configuration()\n",
      "        self.config = extend_config(self.config, kwargs)\n",
      "\n",
      "        self.extractor = ContentExtractor(self.config)\n",
      "\n",
      "        if source_url == '':\n",
      "            scheme = urls.get_scheme(url)\n",
      "            if scheme is None:\n",
      "                scheme = 'http'\n",
      "            source_url = scheme + '://' + urls.get_domain(url)\n",
      "\n",
      "        if source_url is None or source_url == '':\n",
      "            raise ArticleException('input url bad format')\n",
      "\n",
      "        # URL to the main page of the news source which owns this article\n",
      "        self.source_url = source_url\n",
      "\n",
      "        self.url = urls.prepare_url(url, self.source_url)\n",
      "\n",
      "        self.title = title\n",
      "\n",
      "        # URL of the \"best image\" to represent this article\n",
      "        self.top_img = self.top_image = ''\n",
      "\n",
      "        # stores image provided by metadata\n",
      "        self.meta_img = ''\n",
      "\n",
      "        # All image urls in this article\n",
      "        self.imgs = self.images = []\n",
      "\n",
      "        # All videos in this article: youtube, vimeo, etc\n",
      "        self.movies = []\n",
      "\n",
      "        # Body text from this article\n",
      "        self.text = ''\n",
      "\n",
      "        # `keywords` are extracted via nlp() from the body text\n",
      "        self.keywords = []\n",
      "\n",
      "        # `meta_keywords` are extracted via parse() from <meta> tags\n",
      "        self.meta_keywords = []\n",
      "\n",
      "        # `tags` are also extracted via parse() from <meta> tags\n",
      "        self.tags = set()\n",
      "\n",
      "        # List of authors who have published the article, via parse()\n",
      "        self.authors = []\n",
      "\n",
      "        self.publish_date = ''\n",
      "\n",
      "        # Summary generated from the article's body txt\n",
      "        self.summary = ''\n",
      "\n",
      "        # This article's unchanged and raw HTML\n",
      "        self.html = ''\n",
      "\n",
      "        # The HTML of this article's main node (most important part)\n",
      "        self.article_html = ''\n",
      "\n",
      "        # Keep state for downloads and parsing\n",
      "        self.is_parsed = False\n",
      "        self.download_state = ArticleDownloadState.NOT_STARTED\n",
      "        self.download_exception_msg = None\n",
      "\n",
      "        # Meta description field in the HTML source\n",
      "        self.meta_description = \"\"\n",
      "\n",
      "        # Meta language field in HTML source\n",
      "        self.meta_lang = \"\"\n",
      "\n",
      "        # Meta favicon field in HTML source\n",
      "        self.meta_favicon = \"\"\n",
      "\n",
      "        # Meta tags contain a lot of structured data, e.g. OpenGraph\n",
      "        self.meta_data = {}\n",
      "\n",
      "        # The canonical link of this article if found in the meta data\n",
      "        self.canonical_link = \"\"\n",
      "\n",
      "        # Holds the top element of the DOM that we determine is a candidate\n",
      "        # for the main body of the article\n",
      "        self.top_node = None\n",
      "\n",
      "        # A deepcopied clone of the above object before heavy parsing\n",
      "        # operations, useful for users to query data in the\n",
      "        # \"most important part of the page\"\n",
      "        self.clean_top_node = None\n",
      "\n",
      "        # lxml DOM object generated from HTML\n",
      "        self.doc = None\n",
      "\n",
      "        # A deepcopied clone of the above object before undergoing heavy\n",
      "        # cleaning operations, serves as an API if users need to query the DOM\n",
      "        self.clean_doc = None\n",
      "\n",
      "        # A property dict for users to store custom data.\n",
      "        self.additional_data = {}\n",
      "\n",
      "    def build(self):\n",
      "        \"\"\"Build a lone article from a URL independent of the source (newspaper).\n",
      "        Don't normally call this method b/c it's good to multithread articles\n",
      "        on a source (newspaper) level.\n",
      "        \"\"\"\n",
      "        self.download()\n",
      "        self.parse()\n",
      "        self.nlp()\n",
      "\n",
      "    def download(self, input_html=None, title=None, recursion_counter=0):\n",
      "        \"\"\"Downloads the link's HTML content, don't use if you are batch async\n",
      "        downloading articles\n",
      "\n",
      "        recursion_counter (currently 1) stops refreshes that are potentially\n",
      "        infinite\n",
      "        \"\"\"\n",
      "        if input_html is None:\n",
      "            try:\n",
      "                html = network.get_html_2XX_only(self.url, self.config)\n",
      "            except requests.exceptions.RequestException as e:\n",
      "                self.download_state = ArticleDownloadState.FAILED_RESPONSE\n",
      "                self.download_exception_msg = str(e)\n",
      "                log.debug('Download failed on URL %s because of %s' %\n",
      "                          (self.url, self.download_exception_msg))\n",
      "                return\n",
      "        else:\n",
      "            html = input_html\n",
      "\n",
      "        if self.config.follow_meta_refresh:\n",
      "            meta_refresh_url = extract_meta_refresh(html)\n",
      "            if meta_refresh_url and recursion_counter < 1:\n",
      "                return self.download(\n",
      "                    input_html=network.get_html(meta_refresh_url),\n",
      "                    recursion_counter=recursion_counter + 1)\n",
      "\n",
      "        self.set_html(html)\n",
      "        self.set_title(title)\n",
      "\n",
      "    def parse(self):\n",
      "        self.throw_if_not_downloaded_verbose()\n",
      "\n",
      "        self.doc = self.config.get_parser().fromstring(self.html)\n",
      "        self.clean_doc = copy.deepcopy(self.doc)\n",
      "\n",
      "        if self.doc is None:\n",
      "            # `parse` call failed, return nothing\n",
      "            return\n",
      "\n",
      "        # TODO: Fix this, sync in our fix_url() method\n",
      "        parse_candidate = self.get_parse_candidate()\n",
      "        self.link_hash = parse_candidate.link_hash  # MD5\n",
      "\n",
      "        document_cleaner = DocumentCleaner(self.config)\n",
      "        output_formatter = OutputFormatter(self.config)\n",
      "\n",
      "        title = self.extractor.get_title(self.clean_doc)\n",
      "        self.set_title(title)\n",
      "\n",
      "        authors = self.extractor.get_authors(self.clean_doc)\n",
      "        self.set_authors(authors)\n",
      "\n",
      "        meta_lang = self.extractor.get_meta_lang(self.clean_doc)\n",
      "        self.set_meta_language(meta_lang)\n",
      "\n",
      "        if self.config.use_meta_language:\n",
      "            self.extractor.update_language(self.meta_lang)\n",
      "            output_formatter.update_language(self.meta_lang)\n",
      "\n",
      "        meta_favicon = self.extractor.get_favicon(self.clean_doc)\n",
      "        self.set_meta_favicon(meta_favicon)\n",
      "\n",
      "        meta_description = \\\n",
      "            self.extractor.get_meta_description(self.clean_doc)\n",
      "        self.set_meta_description(meta_description)\n",
      "\n",
      "        canonical_link = self.extractor.get_canonical_link(\n",
      "            self.url, self.clean_doc)\n",
      "        self.set_canonical_link(canonical_link)\n",
      "\n",
      "        tags = self.extractor.extract_tags(self.clean_doc)\n",
      "        self.set_tags(tags)\n",
      "\n",
      "        meta_keywords = self.extractor.get_meta_keywords(\n",
      "            self.clean_doc)\n",
      "        self.set_meta_keywords(meta_keywords)\n",
      "\n",
      "        meta_data = self.extractor.get_meta_data(self.clean_doc)\n",
      "        self.set_meta_data(meta_data)\n",
      "\n",
      "        self.publish_date = self.extractor.get_publishing_date(\n",
      "            self.url,\n",
      "            self.clean_doc)\n",
      "\n",
      "        # Before any computations on the body, clean DOM object\n",
      "        self.doc = document_cleaner.clean(self.doc)\n",
      "\n",
      "        self.top_node = self.extractor.calculate_best_node(self.doc)\n",
      "        if self.top_node is not None:\n",
      "            video_extractor = VideoExtractor(self.config, self.top_node)\n",
      "            self.set_movies(video_extractor.get_videos())\n",
      "\n",
      "            self.top_node = self.extractor.post_cleanup(self.top_node)\n",
      "            self.clean_top_node = copy.deepcopy(self.top_node)\n",
      "\n",
      "            text, article_html = output_formatter.get_formatted(\n",
      "                self.top_node)\n",
      "            self.set_article_html(article_html)\n",
      "            self.set_text(text)\n",
      "\n",
      "        self.fetch_images()\n",
      "\n",
      "        self.is_parsed = True\n",
      "        self.release_resources()\n",
      "\n",
      "    def fetch_images(self):\n",
      "        if self.clean_doc is not None:\n",
      "            meta_img_url = self.extractor.get_meta_img_url(\n",
      "                self.url, self.clean_doc)\n",
      "            self.set_meta_img(meta_img_url)\n",
      "\n",
      "            imgs = self.extractor.get_img_urls(self.url, self.clean_doc)\n",
      "            if self.meta_img:\n",
      "                imgs.add(self.meta_img)\n",
      "            self.set_imgs(imgs)\n",
      "\n",
      "        if self.clean_top_node is not None and not self.has_top_image():\n",
      "            first_img = self.extractor.get_first_img_url(\n",
      "                self.url, self.clean_top_node)\n",
      "            if self.config.fetch_images:\n",
      "                self.set_top_img(first_img)\n",
      "            else:\n",
      "                self.set_top_img_no_check(first_img)\n",
      "\n",
      "        if not self.has_top_image() and self.config.fetch_images:\n",
      "            self.set_reddit_top_img()\n",
      "\n",
      "    def has_top_image(self):\n",
      "        return self.top_img is not None and self.top_img != ''\n",
      "\n",
      "    def is_valid_url(self):\n",
      "        \"\"\"Performs a check on the url of this link to determine if article\n",
      "        is a real news article or not\n",
      "        \"\"\"\n",
      "        return urls.valid_url(self.url)\n",
      "\n",
      "    def is_valid_body(self):\n",
      "        \"\"\"If the article's body text is long enough to meet\n",
      "        standard article requirements, keep the article\n",
      "        \"\"\"\n",
      "        if not self.is_parsed:\n",
      "            raise ArticleException('must parse article before checking \\\n",
      "                                    if it\\'s body is valid!')\n",
      "        meta_type = self.extractor.get_meta_type(self.clean_doc)\n",
      "        wordcount = self.text.split(' ')\n",
      "        sentcount = self.text.split('.')\n",
      "\n",
      "        if (meta_type == 'article' and len(wordcount) >\n",
      "                (self.config.MIN_WORD_COUNT)):\n",
      "            log.debug('%s verified for article and wc' % self.url)\n",
      "            return True\n",
      "\n",
      "        if not self.is_media_news() and not self.text:\n",
      "            log.debug('%s caught for no media no text' % self.url)\n",
      "            return False\n",
      "\n",
      "        if self.title is None or len(self.title.split(' ')) < 2:\n",
      "            log.debug('%s caught for bad title' % self.url)\n",
      "            return False\n",
      "\n",
      "        if len(wordcount) < self.config.MIN_WORD_COUNT:\n",
      "            log.debug('%s caught for word cnt' % self.url)\n",
      "            return False\n",
      "\n",
      "        if len(sentcount) < self.config.MIN_SENT_COUNT:\n",
      "            log.debug('%s caught for sent cnt' % self.url)\n",
      "            return False\n",
      "\n",
      "        if self.html is None or self.html == '':\n",
      "            log.debug('%s caught for no html' % self.url)\n",
      "            return False\n",
      "\n",
      "        log.debug('%s verified for default true' % self.url)\n",
      "        return True\n",
      "\n",
      "    def is_media_news(self):\n",
      "        \"\"\"If the article is related heavily to media:\n",
      "        gallery, video, big pictures, etc\n",
      "        \"\"\"\n",
      "        safe_urls = ['/video', '/slide', '/gallery', '/powerpoint',\n",
      "                     '/fashion', '/glamour', '/cloth']\n",
      "        for s in safe_urls:\n",
      "            if s in self.url:\n",
      "                return True\n",
      "        return False\n",
      "\n",
      "    def nlp(self):\n",
      "        \"\"\"Keyword extraction wrapper\n",
      "        \"\"\"\n",
      "        self.throw_if_not_downloaded_verbose()\n",
      "        self.throw_if_not_parsed_verbose()\n",
      "\n",
      "        nlp.load_stopwords(self.config.get_language())\n",
      "        text_keyws = list(nlp.keywords(self.text).keys())\n",
      "        title_keyws = list(nlp.keywords(self.title).keys())\n",
      "        keyws = list(set(title_keyws + text_keyws))\n",
      "        self.set_keywords(keyws)\n",
      "\n",
      "        max_sents = self.config.MAX_SUMMARY_SENT\n",
      "\n",
      "        summary_sents = nlp.summarize(title=self.title, text=self.text, max_sents=max_sents)\n",
      "        summary = '\\n'.join(summary_sents)\n",
      "        self.set_summary(summary)\n",
      "\n",
      "    def get_parse_candidate(self):\n",
      "        \"\"\"A parse candidate is a wrapper object holding a link hash of this\n",
      "        article and a final_url of the article\n",
      "        \"\"\"\n",
      "        if self.html:\n",
      "            return RawHelper.get_parsing_candidate(self.url, self.html)\n",
      "        return URLHelper.get_parsing_candidate(self.url)\n",
      "\n",
      "    def build_resource_path(self):\n",
      "        \"\"\"Must be called after computing HTML/final URL\n",
      "        \"\"\"\n",
      "        res_path = self.get_resource_path()\n",
      "        if not os.path.exists(res_path):\n",
      "            os.mkdir(res_path)\n",
      "\n",
      "    def get_resource_path(self):\n",
      "        \"\"\"Every article object has a special directory to store data in from\n",
      "        initialization to garbage collection\n",
      "        \"\"\"\n",
      "        res_dir_fn = 'article_resources'\n",
      "        resource_directory = os.path.join(settings.TOP_DIRECTORY, res_dir_fn)\n",
      "        if not os.path.exists(resource_directory):\n",
      "            os.mkdir(resource_directory)\n",
      "        dir_path = os.path.join(resource_directory, '%s_' % self.link_hash)\n",
      "        return dir_path\n",
      "\n",
      "    def release_resources(self):\n",
      "        # TODO: implement in entirety\n",
      "        path = self.get_resource_path()\n",
      "        for fname in glob.glob(path):\n",
      "            try:\n",
      "                os.remove(fname)\n",
      "            except OSError:\n",
      "                pass\n",
      "        # os.remove(path)\n",
      "\n",
      "    def set_reddit_top_img(self):\n",
      "        \"\"\"Wrapper for setting images. Queries known image attributes\n",
      "        first, then uses Reddit's image algorithm as a fallback.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            s = images.Scraper(self)\n",
      "            self.set_top_img(s.largest_image_url())\n",
      "        except TypeError as e:\n",
      "            if \"Can't convert 'NoneType' object to str implicitly\" in e.args[0]:\n",
      "                log.debug('No pictures found. Top image not set, %s' % e)\n",
      "            elif 'timed out' in e.args[0]:\n",
      "                log.debug('Download of picture timed out. Top image not set, %s' % e)\n",
      "            else:\n",
      "                log.critical('TypeError other than None type error. '\n",
      "                             'Cannot set top image using the Reddit '\n",
      "                             'algorithm. Possible error with PIL., %s' % e)\n",
      "        except Exception as e:\n",
      "            log.critical('Other error with setting top image using the '\n",
      "                         'Reddit algorithm. Possible error with PIL, %s' % e)\n",
      "\n",
      "    def set_title(self, input_title):\n",
      "        if input_title:\n",
      "            self.title = input_title[:self.config.MAX_TITLE]\n",
      "\n",
      "    def set_text(self, text):\n",
      "        text = text[:self.config.MAX_TEXT]\n",
      "        if text:\n",
      "            self.text = text\n",
      "\n",
      "    def set_html(self, html):\n",
      "        \"\"\"Encode HTML before setting it\n",
      "        \"\"\"\n",
      "        if html:\n",
      "            if isinstance(html, bytes):\n",
      "                html = self.config.get_parser().get_unicode_html(html)\n",
      "            self.html = html\n",
      "            self.download_state = ArticleDownloadState.SUCCESS\n",
      "\n",
      "    def set_article_html(self, article_html):\n",
      "        \"\"\"Sets the HTML of just the article's `top_node`\n",
      "        \"\"\"\n",
      "        if article_html:\n",
      "            self.article_html = article_html\n",
      "\n",
      "    def set_meta_img(self, src_url):\n",
      "        self.meta_img = src_url\n",
      "        self.set_top_img_no_check(src_url)\n",
      "\n",
      "    def set_top_img(self, src_url):\n",
      "        if src_url is not None:\n",
      "            s = images.Scraper(self)\n",
      "            if s.satisfies_requirements(src_url):\n",
      "                self.set_top_img_no_check(src_url)\n",
      "\n",
      "    def set_top_img_no_check(self, src_url):\n",
      "        \"\"\"Provide 2 APIs for images. One at \"top_img\", \"imgs\"\n",
      "        and one at \"top_image\", \"images\"\n",
      "        \"\"\"\n",
      "        self.top_img = src_url\n",
      "        self.top_image = src_url\n",
      "\n",
      "    def set_imgs(self, imgs):\n",
      "        \"\"\"The motive for this method is the same as above, provide APIs\n",
      "        for both `article.imgs` and `article.images`\n",
      "        \"\"\"\n",
      "        self.images = imgs\n",
      "        self.imgs = imgs\n",
      "\n",
      "    def set_keywords(self, keywords):\n",
      "        \"\"\"Keys are stored in list format\n",
      "        \"\"\"\n",
      "        if not isinstance(keywords, list):\n",
      "            raise Exception(\"Keyword input must be list!\")\n",
      "        if keywords:\n",
      "            self.keywords = keywords[:self.config.MAX_KEYWORDS]\n",
      "\n",
      "    def set_authors(self, authors):\n",
      "        \"\"\"Authors are in [\"firstName lastName\", \"firstName lastName\"] format\n",
      "        \"\"\"\n",
      "        if not isinstance(authors, list):\n",
      "            raise Exception(\"authors input must be list!\")\n",
      "        if authors:\n",
      "            self.authors = authors[:self.config.MAX_AUTHORS]\n",
      "\n",
      "    def set_summary(self, summary):\n",
      "        \"\"\"Summary here refers to a paragraph of text from the\n",
      "        title text and body text\n",
      "        \"\"\"\n",
      "        self.summary = summary[:self.config.MAX_SUMMARY]\n",
      "\n",
      "    def set_meta_language(self, meta_lang):\n",
      "        \"\"\"Save langauges in their ISO 2-character form\n",
      "        \"\"\"\n",
      "        if meta_lang and len(meta_lang) >= 2 and \\\n",
      "           meta_lang in get_available_languages():\n",
      "            self.meta_lang = meta_lang[:2]\n",
      "\n",
      "    def set_meta_keywords(self, meta_keywords):\n",
      "        \"\"\"Store the keys in list form\n",
      "        \"\"\"\n",
      "        self.meta_keywords = [k.strip() for k in meta_keywords.split(',')]\n",
      "\n",
      "    def set_meta_favicon(self, meta_favicon):\n",
      "        self.meta_favicon = meta_favicon\n",
      "\n",
      "    def set_meta_description(self, meta_description):\n",
      "        self.meta_description = meta_description\n",
      "\n",
      "    def set_meta_data(self, meta_data):\n",
      "        self.meta_data = meta_data\n",
      "\n",
      "    def set_canonical_link(self, canonical_link):\n",
      "        self.canonical_link = canonical_link\n",
      "\n",
      "    def set_tags(self, tags):\n",
      "        self.tags = tags\n",
      "\n",
      "    def set_movies(self, movie_objects):\n",
      "        \"\"\"Trim video objects into just urls\n",
      "        \"\"\"\n",
      "        movie_urls = [o.src for o in movie_objects if o and o.src]\n",
      "        self.movies = movie_urls\n",
      "\n",
      "    def throw_if_not_downloaded_verbose(self):\n",
      "        \"\"\"Parse ArticleDownloadState -> log readable status\n",
      "        -> maybe throw ArticleException\n",
      "        \"\"\"\n",
      "        if self.download_state == ArticleDownloadState.NOT_STARTED:\n",
      "            raise ArticleException('You must `download()` an article first!')\n",
      "        elif self.download_state == ArticleDownloadState.FAILED_RESPONSE:\n",
      "            raise ArticleException('Article `download()` failed with %s on URL %s' %\n",
      "                  (self.download_exception_msg, self.url))\n",
      "\n",
      "    def throw_if_not_parsed_verbose(self):\n",
      "        \"\"\"Parse `is_parsed` status -> log readable status\n",
      "        -> maybe throw ArticleException\n",
      "        \"\"\"\n",
      "        if not self.is_parsed:\n",
      "            raise ArticleException('You must `parse()` an article first!')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect \n",
    "from newspaper import Article\n",
    "\n",
    "# inspect newspaper Article function\n",
    "print(inspect.getsource(Article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Union\n",
    "\n",
    "# Define types for mean function, trying to analyze input possibilities\n",
    "Number = Union[int, float]  # Number can be either int or float type\n",
    "Numbers = list[Number] # Numbers is a list of Number types\n",
    "Scores = Union[Number, Numbers] # Scores can be single or multiple \n",
    "\n",
    "def mean(scores: Scores, method: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mean of a list of scores.\n",
    "    \n",
    "    Average and Average2 are hidden functions performing mean algorithm\n",
    "\n",
    "    If a single score is provided in scores, it is returned as the mean.\n",
    "    If a list of scores is provided, the average is calculated and returned.\n",
    "    \"\"\"\n",
    "    \n",
    "    def average(scores): \n",
    "        \"\"\"Calculate the average of a list of scores using a Python for loop with rounding.\"\"\"\n",
    "        sum = 0\n",
    "        len = 0\n",
    "        for score in scores:\n",
    "            if isinstance(score, Number):\n",
    "                sum += score\n",
    "                len += 1\n",
    "            else:\n",
    "                print(\"Bad data: \" + str(score) + \" in \" + str(scores))\n",
    "                sys.exit()\n",
    "        return sum / len\n",
    "    \n",
    "    def average2(scores):\n",
    "        \"\"\"Calculate the average of a list of scores using the built-in sum() function with rounding.\"\"\"\n",
    "        return sum(scores) / len(scores)\n",
    "\n",
    "    # test to see if scores is  a list of numbers\n",
    "    if isinstance(scores, list):\n",
    "        if method == 1:  \n",
    "            # long method\n",
    "            result = average(scores)\n",
    "        else:\n",
    "            # built in method\n",
    "            result = average2(scores)\n",
    "        return round(result + 0.005, 2)\n",
    "    \n",
    "    return scores # case where scores is a single valu\n",
    "\n",
    "# try with one number\n",
    "singleScore = 100\n",
    "print(\"Print test data: \" + str(singleScore))  # concat data for single line\n",
    "print(\"Mean of single number: \" + str(mean(singleScore)))\n",
    "\n",
    "print()\n",
    "\n",
    "# define a list of numbers\n",
    "testScores = [90.5, 100, 85.4, 88]\n",
    "print(\"Print test data: \" + str(testScores))\n",
    "print(\"Average score, loop method: \" + str(mean(testScores)))\n",
    "print(\"Average score, function method: \" +  str(mean(testScores, 2)))\n",
    "\n",
    "print()\n",
    "\n",
    "badData = [100, \"NaN\", 90]\n",
    "print(\"Print test data: \" + str(badData))\n",
    "print(\"Mean with bad data: \" + str(mean(badData)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Hack\n",
    "Combines all previous tasks into one new project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç Starting News Analysis Project! üì∞ üòÄ\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Apple iPhone 16, AirPods 4 and the Apple Watch 10: Everything you missed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "CNN ‚Äî\n",
       "\n",
       "Apple on Monday unveiled a host of new technology, including the iPhone 16, which features some impressive updates the company hopes will convince customers ‚Äì many of whom have been holding onto their older gadgets ‚Äì to upgrade.\n",
       "\n",
       "Since the 2020 launch of the iPhone 12, which was the first Apple smartphone with 5G connectivity, the company has given customers few reasons to buy the latest generation. Phone cameras have largely advanced to the point where they‚Äôre sufficient for most people‚Äôs day-to-day needs without major hardware changes, and at some point the human eye isn‚Äôt even capable of perceiving higher screen resolutions.\n",
       "\n",
       "That‚Äôs why roughly 300 million iPhones worldwide have not been upgraded in more than four years, according to a research note from analyst Dan Ives of investment firm Wedbush last month. As a result, iPhone sales, which make up half of the company‚Äôs revenue, have been sluggish.\n",
       "\n",
       "Similarly, the Apple Watch and AirPods have seen mostly incremental upgrades..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for 'Apple iPhone 16, AirPods 4 and the Apple Watch 10: Everything you missed': 1205\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Nvidia is suddenly in trouble"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "CNN ‚Äî\n",
       "\n",
       "Nvidia, the AI chipmaking titan that was briefly the world‚Äôs most valuable company, has suddenly found itself in an unfamiliar position: a major rut.\n",
       "\n",
       "Nvidia (NVDA) had the worst day in the history of the stock market Tuesday, as measured by loss in total market value. Its 9.5% share price decline shaved a stunning $279 billion off the company‚Äôs value, far outpacing the previous record of $240 billion set by Meta in 2022.\n",
       "\n",
       "To put that shocking decline into context, only 27 companies on the planet are worth as much as Nvidia lost in value Tuesday. That $279 billion evaporation is worth more than all the shares of some of America‚Äôs biggest companies, including McDonald‚Äôs, Chevron and Pepsi.\n",
       "\n",
       "CEO Jensen Huang, who is Nvidia‚Äôs largest individual shareholder (and fifth-largest overall, counting institutional investors like BlackRock) personally lost $10 billion in wealth Tuesday from Nvidia‚Äôs sharp tumble.\n",
       "\n",
       "The company has been in decline since June 18, when it topped $3.3 trillion i..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for 'Nvidia is suddenly in trouble': 688\n",
      "\n",
      "Calculating statistics...\n",
      "Average word count across articles: 946.5\n",
      "\n",
      "Inspecting the Article class structure...\n",
      "class Article(object):\n",
      "    \"\"\"Article objects abstract an online news article page\n",
      "    \"\"\"\n",
      "    def __init__(self, url, title='', source_url='', config=None, **kwargs):\n",
      "        \"\"\"The **kwargs argument may be filled with config values, which\n",
      "        is added into the config object\n",
      "        \"\"\"\n",
      "        if isinstance(title, Configuration) or \\\n",
      "                isinstance(source_url, Configuration):\n",
      "            raise ArticleException(\n",
      "                'Configuration object being passed incorrectly as title or '\n",
      "                'source_url! Please verify `Article`s __init__() fn.')\n",
      "\n",
      "        self.config = config or Configuration()\n",
      "        self.config = extend_config(self.config, kwargs)\n",
      "\n",
      "        self.extractor = ContentExtractor(self.config)\n",
      "\n",
      "        if source_url == '':\n",
      "            scheme = urls.get_scheme(url)\n",
      "            if scheme is None:\n",
      "                scheme = 'http'\n",
      "            source_url = scheme + '://' + urls.get_domain(url)\n",
      "\n",
      "        if source_url is None or source_url == '':\n",
      "            raise ArticleException('input url bad format')\n",
      "\n",
      "        # URL to the main page of the news source which owns this article\n",
      "        self.source_url = source_url\n",
      "\n",
      "        self.url = urls.prepare_url(url, self.source_url)\n",
      "\n",
      "        self.title = title\n",
      "\n",
      "        # URL of the \"best image\" to represent this article\n",
      "        self.top_img = self.top_image = ''\n",
      "\n",
      "        # stores image provided by metadata\n",
      "        self.meta_img = ''\n",
      "\n",
      "        # All image urls in this article\n",
      "        self.imgs = self.images = []\n",
      "\n",
      "        # All videos in this article: youtube, vimeo, etc\n",
      "        self.movies = []\n",
      "\n",
      "        # Body text from this article\n",
      "        self.text = ''\n",
      "\n",
      "        # `keywords` are extracted via nlp() from the body text\n",
      "        self.keywords = []\n",
      "\n",
      "        # `meta_keywords` are extracted via parse() from <meta> tags\n",
      "        self.meta_keywords = []\n",
      "\n",
      "        # `tags` are also extracted via parse() from <meta> tags\n",
      "        self.tags = set()\n",
      "\n",
      "        # List of authors who have published the article, via parse()\n",
      "        self.authors = []\n",
      "\n",
      "        self.publish_date = ''\n",
      "\n",
      "        # Summary generated from the article's body txt\n",
      "        self.summary = ''\n",
      "\n",
      "        # This article's unchanged and raw HTML\n",
      "        self.html = ''\n",
      "\n",
      "        # The HTML of this article's main node (most important part)\n",
      "        self.article_html = ''\n",
      "\n",
      "        # Keep state for downloads and parsing\n",
      "        self.is_parsed = False\n",
      "        self.download_state = ArticleDownloadState.NOT_STARTED\n",
      "        self.download_exception_msg = None\n",
      "\n",
      "        # Meta description field in the HTML source\n",
      "        self.meta_description = \"\"\n",
      "\n",
      "        # Meta language field in HTML source\n",
      "        self.meta_lang = \"\"\n",
      "\n",
      "        # Meta favicon field in HTML source\n",
      "        self.meta_favicon = \"\"\n",
      "\n",
      "        # Meta tags contain a lot of structured data, e.g. OpenGraph\n",
      "        self.meta_data = {}\n",
      "\n",
      "        # The canonical link of this article if found in the meta data\n",
      "        self.canonical_link = \"\"\n",
      "\n",
      "        # Holds the top element of the DOM that we determine is a candidate\n",
      "        # for the main body of the article\n",
      "        self.top_node = None\n",
      "\n",
      "        # A deepcopied clone of the above object before heavy parsing\n",
      "        # operations, useful for users to query data in the\n",
      "        # \"most important part of the page\"\n",
      "        self.clean_top_node = None\n",
      "\n",
      "        # lxml DOM object generated from HTML\n",
      "        self.doc = None\n",
      "\n",
      "        # A deepcopied clone of the above object before undergoing heavy\n",
      "        # cleaning operations, serves as an API if users need to query the DOM\n",
      "        self.clean_doc = None\n",
      "\n",
      "        # A property dict for users to store custom data.\n",
      "        self.additional_data = {}\n",
      "\n",
      "    def build(self):\n",
      "        \"\"\"Build a lone article from a URL independent of the source (newspaper).\n",
      "        Don't normally call this method b/c it's good to multithread articles\n",
      "        on a source (newspaper) level.\n",
      "        \"\"\"\n",
      "        self.download()\n",
      "        self.parse()\n",
      "        self.nlp()\n",
      "\n",
      "    def download(self, input_html=None, title=None, recursion_counter=0):\n",
      "        \"\"\"Downloads the link's HTML content, don't use if you are batch async\n",
      "        downloading articles\n",
      "\n",
      "        recursion_counter (currently 1) stops refreshes that are potentially\n",
      "        infinite\n",
      "        \"\"\"\n",
      "        if input_html is None:\n",
      "            try:\n",
      "                html = network.get_html_2XX_only(self.url, self.config)\n",
      "            except requests.exceptions.RequestException as e:\n",
      "                self.download_state = ArticleDownloadState.FAILED_RESPONSE\n",
      "                self.download_exception_msg = str(e)\n",
      "                log.debug('Download failed on URL %s because of %s' %\n",
      "                          (self.url, self.download_exception_msg))\n",
      "                return\n",
      "        else:\n",
      "            html = input_html\n",
      "\n",
      "        if self.config.follow_meta_refresh:\n",
      "            meta_refresh_url = extract_meta_refresh(html)\n",
      "            if meta_refresh_url and recursion_counter < 1:\n",
      "                return self.download(\n",
      "                    input_html=network.get_html(meta_refresh_url),\n",
      "                    recursion_counter=recursion_counter + 1)\n",
      "\n",
      "        self.set_html(html)\n",
      "        self.set_title(title)\n",
      "\n",
      "    def parse(self):\n",
      "        self.throw_if_not_downloaded_verbose()\n",
      "\n",
      "        self.doc = self.config.get_parser().fromstring(self.html)\n",
      "        self.clean_doc = copy.deepcopy(self.doc)\n",
      "\n",
      "        if self.doc is None:\n",
      "            # `parse` call failed, return nothing\n",
      "            return\n",
      "\n",
      "        # TODO: Fix this, sync in our fix_url() method\n",
      "        parse_candidate = self.get_parse_candidate()\n",
      "        self.link_hash = parse_candidate.link_hash  # MD5\n",
      "\n",
      "        document_cleaner = DocumentCleaner(self.config)\n",
      "        output_formatter = OutputFormatter(self.config)\n",
      "\n",
      "        title = self.extractor.get_title(self.clean_doc)\n",
      "        self.set_title(title)\n",
      "\n",
      "        authors = self.extractor.get_authors(self.clean_doc)\n",
      "        self.set_authors(authors)\n",
      "\n",
      "        meta_lang = self.extractor.get_meta_lang(self.clean_doc)\n",
      "        self.set_meta_language(meta_lang)\n",
      "\n",
      "        if self.config.use_meta_language:\n",
      "            self.extractor.update_language(self.meta_lang)\n",
      "            output_formatter.update_language(self.meta_lang)\n",
      "\n",
      "        meta_favicon = self.extractor.get_favicon(self.clean_doc)\n",
      "        self.set_meta_favicon(meta_favicon)\n",
      "\n",
      "        meta_description = \\\n",
      "            self.extractor.get_meta_description(self.clean_doc)\n",
      "        self.set_meta_description(meta_description)\n",
      "\n",
      "        canonical_link = self.extractor.get_canonical_link(\n",
      "            self.url, self.clean_doc)\n",
      "        self.set_canonical_link(canonical_link)\n",
      "\n",
      "        tags = self.extractor.extract_tags(self.clean_doc)\n",
      "        self.set_tags(tags)\n",
      "\n",
      "        meta_keywords = self.extractor.get_meta_keywords(\n",
      "            self.clean_doc)\n",
      "        self.set_meta_keywords(meta_keywords)\n",
      "\n",
      "        meta_data = self.extractor.get_meta_data(self.clean_doc)\n",
      "        self.set_meta_data(meta_data)\n",
      "\n",
      "        self.publish_date = self.extractor.get_publishing_date(\n",
      "            self.url,\n",
      "            self.clean_doc)\n",
      "\n",
      "        # Before any computations on the body, clean DOM object\n",
      "        self.doc = document_cleaner.clean(self.doc)\n",
      "\n",
      "        self.top_node = self.extractor.calculate_best_node(self.doc)\n",
      "        if self.top_node is not None:\n",
      "            video_extractor = VideoExtractor(self.config, self.top_node)\n",
      "            self.set_movies(video_extractor.get_videos())\n",
      "\n",
      "            self.top_node = self.extractor.post_cleanup(self.top_node)\n",
      "            self.clean_top_node = copy.deepcopy(self.top_node)\n",
      "\n",
      "            text, article_html = output_formatter.get_formatted(\n",
      "                self.top_node)\n",
      "            self.set_article_html(article_html)\n",
      "            self.set_text(text)\n",
      "\n",
      "        self.fetch_images()\n",
      "\n",
      "        self.is_parsed = True\n",
      "        self.release_resources()\n",
      "\n",
      "    def fetch_images(self):\n",
      "        if self.clean_doc is not None:\n",
      "            meta_img_url = self.extractor.get_meta_img_url(\n",
      "                self.url, self.clean_doc)\n",
      "            self.set_meta_img(meta_img_url)\n",
      "\n",
      "            imgs = self.extractor.get_img_urls(self.url, self.clean_doc)\n",
      "            if self.meta_img:\n",
      "                imgs.add(self.meta_img)\n",
      "            self.set_imgs(imgs)\n",
      "\n",
      "        if self.clean_top_node is not None and not self.has_top_image():\n",
      "            first_img = self.extractor.get_first_img_url(\n",
      "                self.url, self.clean_top_node)\n",
      "            if self.config.fetch_images:\n",
      "                self.set_top_img(first_img)\n",
      "            else:\n",
      "                self.set_top_img_no_check(first_img)\n",
      "\n",
      "        if not self.has_top_image() and self.config.fetch_images:\n",
      "            self.set_reddit_top_img()\n",
      "\n",
      "    def has_top_image(self):\n",
      "        return self.top_img is not None and self.top_img != ''\n",
      "\n",
      "    def is_valid_url(self):\n",
      "        \"\"\"Performs a check on the url of this link to determine if article\n",
      "        is a real news article or not\n",
      "        \"\"\"\n",
      "        return urls.valid_url(self.url)\n",
      "\n",
      "    def is_valid_body(self):\n",
      "        \"\"\"If the article's body text is long enough to meet\n",
      "        standard article requirements, keep the article\n",
      "        \"\"\"\n",
      "        if not self.is_parsed:\n",
      "            raise ArticleException('must parse article before checking \\\n",
      "                                    if it\\'s body is valid!')\n",
      "        meta_type = self.extractor.get_meta_type(self.clean_doc)\n",
      "        wordcount = self.text.split(' ')\n",
      "        sentcount = self.text.split('.')\n",
      "\n",
      "        if (meta_type == 'article' and len(wordcount) >\n",
      "                (self.config.MIN_WORD_COUNT)):\n",
      "            log.debug('%s verified for article and wc' % self.url)\n",
      "            return True\n",
      "\n",
      "        if not self.is_media_news() and not self.text:\n",
      "            log.debug('%s caught for no media no text' % self.url)\n",
      "            return False\n",
      "\n",
      "        if self.title is None or len(self.title.split(' ')) < 2:\n",
      "            log.debug('%s caught for bad title' % self.url)\n",
      "            return False\n",
      "\n",
      "        if len(wordcount) < self.config.MIN_WORD_COUNT:\n",
      "            log.debug('%s caught for word cnt' % self.url)\n",
      "            return False\n",
      "\n",
      "        if len(sentcount) < self.config.MIN_SENT_COUNT:\n",
      "            log.debug('%s caught for sent cnt' % self.url)\n",
      "            return False\n",
      "\n",
      "        if self.html is None or self.html == '':\n",
      "            log.debug('%s caught for no html' % self.url)\n",
      "            return False\n",
      "\n",
      "        log.debug('%s verified for default true' % self.url)\n",
      "        return True\n",
      "\n",
      "    def is_media_news(self):\n",
      "        \"\"\"If the article is related heavily to media:\n",
      "        gallery, video, big pictures, etc\n",
      "        \"\"\"\n",
      "        safe_urls = ['/video', '/slide', '/gallery', '/powerpoint',\n",
      "                     '/fashion', '/glamour', '/cloth']\n",
      "        for s in safe_urls:\n",
      "            if s in self.url:\n",
      "                return True\n",
      "        return False\n",
      "\n",
      "    def nlp(self):\n",
      "        \"\"\"Keyword extraction wrapper\n",
      "        \"\"\"\n",
      "        self.throw_if_not_downloaded_verbose()\n",
      "        self.throw_if_not_parsed_verbose()\n",
      "\n",
      "        nlp.load_stopwords(self.config.get_language())\n",
      "        text_keyws = list(nlp.keywords(self.text).keys())\n",
      "        title_keyws = list(nlp.keywords(self.title).keys())\n",
      "        keyws = list(set(title_keyws + text_keyws))\n",
      "        self.set_keywords(keyws)\n",
      "\n",
      "        max_sents = self.config.MAX_SUMMARY_SENT\n",
      "\n",
      "        summary_sents = nlp.summarize(title=self.title, text=self.text, max_sents=max_sents)\n",
      "        summary = '\\n'.join(summary_sents)\n",
      "        self.set_summary(summary)\n",
      "\n",
      "    def get_parse_candidate(self):\n",
      "        \"\"\"A parse candidate is a wrapper object holding a link hash of this\n",
      "        article and a final_url of the article\n",
      "        \"\"\"\n",
      "        if self.html:\n",
      "            return RawHelper.get_parsing_candidate(self.url, self.html)\n",
      "        return URLHelper.get_parsing_candidate(self.url)\n",
      "\n",
      "    def build_resource_path(self):\n",
      "        \"\"\"Must be called after computing HTML/final URL\n",
      "        \"\"\"\n",
      "        res_path = self.get_resource_path()\n",
      "        if not os.path.exists(res_path):\n",
      "            os.mkdir(res_path)\n",
      "\n",
      "    def get_resource_path(self):\n",
      "        \"\"\"Every article object has a special directory to store data in from\n",
      "        initialization to garbage collection\n",
      "        \"\"\"\n",
      "        res_dir_fn = 'article_resources'\n",
      "        resource_directory = os.path.join(settings.TOP_DIRECTORY, res_dir_fn)\n",
      "        if not os.path.exists(resource_directory):\n",
      "            os.mkdir(resource_directory)\n",
      "        dir_path = os.path.join(resource_directory, '%s_' % self.link_hash)\n",
      "        return dir_path\n",
      "\n",
      "    def release_resources(self):\n",
      "        # TODO: implement in entirety\n",
      "        path = self.get_resource_path()\n",
      "        for fname in glob.glob(path):\n",
      "            try:\n",
      "                os.remove(fname)\n",
      "            except OSError:\n",
      "                pass\n",
      "        # os.remove(path)\n",
      "\n",
      "    def set_reddit_top_img(self):\n",
      "        \"\"\"Wrapper for setting images. Queries known image attributes\n",
      "        first, then uses Reddit's image algorithm as a fallback.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            s = images.Scraper(self)\n",
      "            self.set_top_img(s.largest_image_url())\n",
      "        except TypeError as e:\n",
      "            if \"Can't convert 'NoneType' object to str implicitly\" in e.args[0]:\n",
      "                log.debug('No pictures found. Top image not set, %s' % e)\n",
      "            elif 'timed out' in e.args[0]:\n",
      "                log.debug('Download of picture timed out. Top image not set, %s' % e)\n",
      "            else:\n",
      "                log.critical('TypeError other than None type error. '\n",
      "                             'Cannot set top image using the Reddit '\n",
      "                             'algorithm. Possible error with PIL., %s' % e)\n",
      "        except Exception as e:\n",
      "            log.critical('Other error with setting top image using the '\n",
      "                         'Reddit algorithm. Possible error with PIL, %s' % e)\n",
      "\n",
      "    def set_title(self, input_title):\n",
      "        if input_title:\n",
      "            self.title = input_title[:self.config.MAX_TITLE]\n",
      "\n",
      "    def set_text(self, text):\n",
      "        text = text[:self.config.MAX_TEXT]\n",
      "        if text:\n",
      "            self.text = text\n",
      "\n",
      "    def set_html(self, html):\n",
      "        \"\"\"Encode HTML before setting it\n",
      "        \"\"\"\n",
      "        if html:\n",
      "            if isinstance(html, bytes):\n",
      "                html = self.config.get_parser().get_unicode_html(html)\n",
      "            self.html = html\n",
      "            self.download_state = ArticleDownloadState.SUCCESS\n",
      "\n",
      "    def set_article_html(self, article_html):\n",
      "        \"\"\"Sets the HTML of just the article's `top_node`\n",
      "        \"\"\"\n",
      "        if article_html:\n",
      "            self.article_html = article_html\n",
      "\n",
      "    def set_meta_img(self, src_url):\n",
      "        self.meta_img = src_url\n",
      "        self.set_top_img_no_check(src_url)\n",
      "\n",
      "    def set_top_img(self, src_url):\n",
      "        if src_url is not None:\n",
      "            s = images.Scraper(self)\n",
      "            if s.satisfies_requirements(src_url):\n",
      "                self.set_top_img_no_check(src_url)\n",
      "\n",
      "    def set_top_img_no_check(self, src_url):\n",
      "        \"\"\"Provide 2 APIs for images. One at \"top_img\", \"imgs\"\n",
      "        and one at \"top_image\", \"images\"\n",
      "        \"\"\"\n",
      "        self.top_img = src_url\n",
      "        self.top_image = src_url\n",
      "\n",
      "    def set_imgs(self, imgs):\n",
      "        \"\"\"The motive for this method is the same as above, provide APIs\n",
      "        for both `article.imgs` and `article.images`\n",
      "        \"\"\"\n",
      "        self.images = imgs\n",
      "        self.imgs = imgs\n",
      "\n",
      "    def set_keywords(self, keywords):\n",
      "        \"\"\"Keys are stored in list format\n",
      "        \"\"\"\n",
      "        if not isinstance(keywords, list):\n",
      "            raise Exception(\"Keyword input must be list!\")\n",
      "        if keywords:\n",
      "            self.keywords = keywords[:self.config.MAX_KEYWORDS]\n",
      "\n",
      "    def set_authors(self, authors):\n",
      "        \"\"\"Authors are in [\"firstName lastName\", \"firstName lastName\"] format\n",
      "        \"\"\"\n",
      "        if not isinstance(authors, list):\n",
      "            raise Exception(\"authors input must be list!\")\n",
      "        if authors:\n",
      "            self.authors = authors[:self.config.MAX_AUTHORS]\n",
      "\n",
      "    def set_summary(self, summary):\n",
      "        \"\"\"Summary here refers to a paragraph of text from the\n",
      "        title text and body text\n",
      "        \"\"\"\n",
      "        self.summary = summary[:self.config.MAX_SUMMARY]\n",
      "\n",
      "    def set_meta_language(self, meta_lang):\n",
      "        \"\"\"Save langauges in their ISO 2-character form\n",
      "        \"\"\"\n",
      "        if meta_lang and len(meta_lang) >= 2 and \\\n",
      "           meta_lang in get_available_languages():\n",
      "            self.meta_lang = meta_lang[:2]\n",
      "\n",
      "    def set_meta_keywords(self, meta_keywords):\n",
      "        \"\"\"Store the keys in list form\n",
      "        \"\"\"\n",
      "        self.meta_keywords = [k.strip() for k in meta_keywords.split(',')]\n",
      "\n",
      "    def set_meta_favicon(self, meta_favicon):\n",
      "        self.meta_favicon = meta_favicon\n",
      "\n",
      "    def set_meta_description(self, meta_description):\n",
      "        self.meta_description = meta_description\n",
      "\n",
      "    def set_meta_data(self, meta_data):\n",
      "        self.meta_data = meta_data\n",
      "\n",
      "    def set_canonical_link(self, canonical_link):\n",
      "        self.canonical_link = canonical_link\n",
      "\n",
      "    def set_tags(self, tags):\n",
      "        self.tags = tags\n",
      "\n",
      "    def set_movies(self, movie_objects):\n",
      "        \"\"\"Trim video objects into just urls\n",
      "        \"\"\"\n",
      "        movie_urls = [o.src for o in movie_objects if o and o.src]\n",
      "        self.movies = movie_urls\n",
      "\n",
      "    def throw_if_not_downloaded_verbose(self):\n",
      "        \"\"\"Parse ArticleDownloadState -> log readable status\n",
      "        -> maybe throw ArticleException\n",
      "        \"\"\"\n",
      "        if self.download_state == ArticleDownloadState.NOT_STARTED:\n",
      "            raise ArticleException('You must `download()` an article first!')\n",
      "        elif self.download_state == ArticleDownloadState.FAILED_RESPONSE:\n",
      "            raise ArticleException('Article `download()` failed with %s on URL %s' %\n",
      "                  (self.download_exception_msg, self.url))\n",
      "\n",
      "    def throw_if_not_parsed_verbose(self):\n",
      "        \"\"\"Parse `is_parsed` status -> log readable status\n",
      "        -> maybe throw ArticleException\n",
      "        \"\"\"\n",
      "        if not self.is_parsed:\n",
      "            raise ArticleException('You must `parse()` an article first!')\n",
      "\n",
      "\n",
      "News Analysis Project Complete! :checkered_flag:\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from newspaper import Article\n",
    "from emoji import emojize\n",
    "import inspect\n",
    "from typing import Union\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Display a friendly message with emojis\n",
    "print(emojize(\":thumbs_up: Starting News Analysis Project! :newspaper: :grinning_face:\"))\n",
    "\n",
    "# Data type union for scores used in calculations later\n",
    "Number = Union[int, float]\n",
    "Numbers = list[Number]\n",
    "Scores = Union[Number, Numbers]\n",
    "\n",
    "# Define a mean function for calculating the average of article word counts\n",
    "def mean(scores: Scores, method: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mean of a list of scores.\n",
    "    If a single score is provided, it is returned as the mean.\n",
    "    \"\"\"\n",
    "    def average(scores): \n",
    "        total = 0\n",
    "        count = 0\n",
    "        for score in scores:\n",
    "            total += score\n",
    "            count += 1\n",
    "        return total / count\n",
    "    \n",
    "    def average2(scores):\n",
    "        return sum(scores) / len(scores)\n",
    "\n",
    "    if isinstance(scores, list):\n",
    "        if method == 1:  \n",
    "            return round(average(scores), 2)\n",
    "        else:\n",
    "            return round(average2(scores), 2)\n",
    "    \n",
    "    return scores  # Return if it's a single number\n",
    "\n",
    "# URLs of articles to analyze\n",
    "urls = [\n",
    "    \"https://www.cnn.com/2024/09/09/tech/new-iphone-16-apple-release/index.html\", \n",
    "    \"https://www.cnn.com/2024/09/04/tech/nvidia-is-in-trouble/index.html\"\n",
    "]\n",
    "\n",
    "# List to store word counts of articles for later statistical analysis\n",
    "word_counts = []\n",
    "\n",
    "# Iterate over each article, analyze, and display data\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Create an Article object\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        # Display article title and text in Markdown format\n",
    "        display(Markdown(f\"### {article.title}\"))\n",
    "        display(Markdown(article.text[:1000] + \"...\"))  # Display only the first 1000 characters\n",
    "\n",
    "        # Append the word count of the article text to the list\n",
    "        word_count = len(article.text.split())\n",
    "        word_counts.append(word_count)\n",
    "\n",
    "        print(f\"Word count for '{article.title}': {word_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article at {url}: {e}\")\n",
    "\n",
    "# Statistical analysis: Calculate the mean word count\n",
    "print(\"\\nCalculating statistics...\")\n",
    "if word_counts:\n",
    "    average_word_count = mean(word_counts)\n",
    "    print(f\"Average word count across articles: {average_word_count}\")\n",
    "else:\n",
    "    print(\"No articles to analyze.\")\n",
    "\n",
    "# Inspect the Article class from the newspaper module for learning purposes\n",
    "print(\"\\nInspecting the Article class structure...\")\n",
    "print(inspect.getsource(Article))\n",
    "\n",
    "print(\"\\nNews Analysis Project Complete! :checkered_flag:\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
